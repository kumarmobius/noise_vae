name: Variational Autoencoder with Noise Injection Trainer V1
description: Train NoisyVAE. Applies noise to input before encoding. Loss = reconstruction loss + beta * KL divergence. Reads full model_config from init checkpoint and saves it inside every output checkpoint.

inputs:
  # Model and data
  - {name: model_input,   type: Model, description: "Initialized model from VAE builder (.pt)"}
  - {name: train_dataset, type: Data,  description: "Training dataset (.pt tensor file)"}
  - {name: val_dataset,   type: Data,  description: "Validation dataset (.pt tensor file)"}

  # Training
  - {name: epochs,      type: Integer, description: "Max training epochs",     default: "100"}
  - {name: batch_size,  type: Integer, description: "Batch size",              default: "32"}
  - {name: num_workers, type: Integer, description: "DataLoader workers",      default: "0"}

  # KL annealing — gradually ramp kl_weight from 0 to target over N epochs
  # Helps avoid posterior collapse early in training
  - {name: kl_annealing_epochs, type: Integer, description: "Epochs to linearly ramp KL weight from 0 to kl_weight (0 = disabled, use kl_weight from builder)", default: "10"}

  # Gradient clipping
  - {name: gradient_clip_enabled,  type: String,  description: "Enable gradient clipping (true/false)", default: "true"}
  - {name: gradient_clip_max_norm, type: Float,   description: "Max gradient norm",                     default: "1.0"}

  # Early stopping
  - {name: early_stopping_patience,  type: Integer, description: "Early stopping patience (epochs)", default: "10"}
  - {name: early_stopping_min_delta, type: Float,   description: "Min improvement to reset patience", default: "0.000001"}

  # LR scheduler
  - {name: scheduler_type,    type: String,  description: "Scheduler: reduce_on_plateau, cosine, step, exponential, none", default: "reduce_on_plateau"}
  - {name: scheduler_factor,  type: Float,   description: "LR reduction factor",    default: "0.5"}
  - {name: scheduler_patience,type: Integer, description: "Scheduler patience",     default: "5"}
  - {name: scheduler_min_lr,  type: Float,   description: "Minimum learning rate",  default: "0.000001"}

  # Logging
  - {name: save_every_n_epochs,    type: Integer, description: "Periodic checkpoint every N epochs (0=disabled)", default: "0"}
  - {name: print_every_n_batches,  type: Integer, description: "Print batch loss every N batches (0=disabled)",   default: "0"}

  # Misc
  - {name: seed, type: Integer, description: "Random seed", default: "42"}

outputs:
  - {name: trained_model,   type: Model, description: "Best model checkpoint (lowest val loss)"}
  - {name: training_history, type: Data, description: "Training history JSON + npz"}

implementation:
  container:
    image: gurpreetgandhi/nesy-factory:v1
    command:
      - python3
      - -u
      - -c
      - |
        import argparse
        import os
        import sys
        import json
        import logging
        import random
        import time
        import numpy as np
        import torch
        import torch.nn as nn
        import torch.optim as optim
        from torch.utils.data import DataLoader, TensorDataset
        from pathlib import Path

        logging.basicConfig(stream=sys.stdout, level=logging.INFO)
        logger = logging.getLogger("vae_trainer")

        parser = argparse.ArgumentParser()
        parser.add_argument("--model_input",             type=str,   required=True)
        parser.add_argument("--train_dataset",           type=str,   required=True)
        parser.add_argument("--val_dataset",             type=str,   required=True)
        parser.add_argument("--epochs",                  type=int,   required=True)
        parser.add_argument("--batch_size",              type=int,   required=True)
        parser.add_argument("--num_workers",             type=int,   required=True)
        parser.add_argument("--kl_annealing_epochs",     type=int,   required=True)
        parser.add_argument("--gradient_clip_enabled",   type=str,   required=True)
        parser.add_argument("--gradient_clip_max_norm",  type=float, required=True)
        parser.add_argument("--early_stopping_patience", type=int,   required=True)
        parser.add_argument("--early_stopping_min_delta",type=float, required=True)
        parser.add_argument("--scheduler_type",          type=str,   required=True)
        parser.add_argument("--scheduler_factor",        type=float, required=True)
        parser.add_argument("--scheduler_patience",      type=int,   required=True)
        parser.add_argument("--scheduler_min_lr",        type=float, required=True)
        parser.add_argument("--save_every_n_epochs",     type=int,   required=True)
        parser.add_argument("--print_every_n_batches",   type=int,   required=True)
        parser.add_argument("--seed",                    type=int,   required=True)
        parser.add_argument("--trained_model",           type=str,   required=True)
        parser.add_argument("--training_history",        type=str,   required=True)
        args = parser.parse_args()

        # ============================================
        # Helpers
        # ============================================
        def str_to_bool(s):
            return str(s).lower() in ["true", "1", "yes"]

        def set_seed(seed):
            random.seed(seed)
            np.random.seed(seed)
            torch.manual_seed(seed)
            torch.cuda.manual_seed(seed)
            torch.cuda.manual_seed_all(seed)
            torch.backends.cudnn.deterministic = True
            torch.backends.cudnn.benchmark = False
            logger.info(f"✓ Seed set: {seed}")

        def get_activation(name):
            activations = {
                "relu":       nn.ReLU(),
                "leaky_relu": nn.LeakyReLU(0.2),
                "elu":        nn.ELU(),
                "selu":       nn.SELU(),
                "gelu":       nn.GELU(),
                "tanh":       nn.Tanh(),
                "sigmoid":    nn.Sigmoid(),
                "swish":      nn.SiLU(),
                "mish":       nn.Mish(),
                "prelu":      nn.PReLU(),
            }
            act = activations.get(name.lower())
            if act is None:
                raise ValueError(f"Unknown activation '{name}'")
            return act

        # ============================================
        # Model Classes — must match builder exactly
        # ============================================
        class VAEEncoder(nn.Module):
            def __init__(self, input_dim, encoder_dims, latent_dim,
                         dropout, activation, use_batch_norm):
                super(VAEEncoder, self).__init__()
                layers   = []
                prev_dim = input_dim
                for idx, units in enumerate(encoder_dims):
                    layers.append(nn.Linear(prev_dim, units))
                    if use_batch_norm:
                        layers.append(nn.BatchNorm1d(units))
                    layers.append(get_activation(activation))
                    if dropout > 0:
                        layers.append(nn.Dropout(dropout))
                    logger.info(f"    Encoder Layer {idx + 1}: {prev_dim} -> {units}")
                    prev_dim = units
                self.shared     = nn.Sequential(*layers)
                self.fc_mu      = nn.Linear(prev_dim, latent_dim)
                self.fc_log_var = nn.Linear(prev_dim, latent_dim)
                logger.info(f"    Encoder mu/log_var : {prev_dim} -> {latent_dim}")

            def forward(self, x):
                h = self.shared(x)
                return self.fc_mu(h), self.fc_log_var(h)

        class VAEDecoder(nn.Module):
            def __init__(self, latent_dim, decoder_dims, input_dim,
                         dropout, activation, use_batch_norm):
                super(VAEDecoder, self).__init__()
                layers   = []
                prev_dim = latent_dim
                for idx, units in enumerate(decoder_dims):
                    layers.append(nn.Linear(prev_dim, units))
                    if use_batch_norm:
                        layers.append(nn.BatchNorm1d(units))
                    layers.append(get_activation(activation))
                    if dropout > 0:
                        layers.append(nn.Dropout(dropout))
                    logger.info(f"    Decoder Layer {idx + 1}: {prev_dim} -> {units}")
                    prev_dim = units
                layers.append(nn.Linear(prev_dim, input_dim))
                logger.info(f"    Decoder Output : {prev_dim} -> {input_dim}")
                self.decoder = nn.Sequential(*layers)

            def forward(self, z):
                return self.decoder(z)

        class NoisyVAE(nn.Module):
            def __init__(self, input_dim, encoder_dims, decoder_dims, latent_dim,
                         encoder_dropout, decoder_dropout,
                         encoder_activation, decoder_activation,
                         encoder_batch_norm, decoder_batch_norm):
                super(NoisyVAE, self).__init__()
                self.input_dim  = input_dim
                self.latent_dim = latent_dim
                self.encoder = VAEEncoder(input_dim, encoder_dims, latent_dim,
                                          encoder_dropout, encoder_activation, encoder_batch_norm)
                self.decoder = VAEDecoder(latent_dim, decoder_dims, input_dim,
                                          decoder_dropout, decoder_activation, decoder_batch_norm)

            def reparameterize(self, mu, log_var):
                if self.training:
                    std = torch.exp(0.5 * log_var)
                    return mu + torch.randn_like(std) * std
                return mu

            def forward(self, x):
                mu, log_var = self.encoder(x)
                z           = self.reparameterize(mu, log_var)
                recon       = self.decoder(z)
                return recon, mu, log_var, z

            def encode(self, x):
                mu, log_var = self.encoder(x)
                return self.reparameterize(mu, log_var), mu, log_var

            def decode(self, z):
                return self.decoder(z)

            def sample(self, n, device):
                return self.decoder(torch.randn(n, self.latent_dim, device=device))

        # ============================================
        # Noise Injection
        # Applied to clean input BEFORE encoding
        # ============================================
        def inject_noise(x, noise_type, noise_factor):
            if noise_factor == 0.0:
                return x
            if noise_type == "gaussian":
                return x + torch.randn_like(x) * noise_factor
            elif noise_type == "dropout":
                mask = torch.rand_like(x) > noise_factor
                return x * mask
            elif noise_type == "uniform":
                return x + (torch.rand_like(x) - 0.5) * 2 * noise_factor
            elif noise_type == "salt_pepper":
                noisy = x.clone()
                mask  = torch.rand_like(x) < noise_factor
                vals  = torch.rand(mask.sum(), device=x.device)
                vals  = vals * (x.max() - x.min()) + x.min()
                noisy[mask] = vals
                return noisy
            else:
                raise ValueError(f"Unknown noise_type: {noise_type}")

        # ============================================
        # VAE Loss
        # total = recon_loss + kl_weight * kl_loss
        # ============================================
        def get_recon_loss_fn(loss_type):
            fns = {
                "mse":       nn.MSELoss(reduction="sum"),
                "l1":        nn.L1Loss(reduction="sum"),
                "smooth_l1": nn.SmoothL1Loss(reduction="sum"),
                "huber":     nn.HuberLoss(reduction="sum", delta=1.0),
            }
            fn = fns.get(loss_type.lower())
            if fn is None:
                raise ValueError(f"Unknown recon_loss_type: {loss_type}")
            return fn

        def vae_loss(recon, x, mu, log_var, recon_loss_fn, kl_weight):
            batch_size  = x.size(0)

            # Reconstruction loss — sum over dims, mean over batch
            recon_loss  = recon_loss_fn(recon, x) / batch_size

            # KL divergence: -0.5 * sum(1 + log_var - mu^2 - exp(log_var))
            # Closed-form solution assuming N(mu, sigma^2) vs N(0,1) prior
            kl_loss     = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp()) / batch_size

            total_loss  = recon_loss + kl_weight * kl_loss
            return total_loss, recon_loss, kl_loss

        # ============================================
        # Scheduler setup
        # ============================================
        def setup_scheduler(optimizer, scheduler_type, factor, patience, min_lr, epochs):
            stype = scheduler_type.lower()
            if stype == "none":
                return None
            elif stype == "reduce_on_plateau":
                return optim.lr_scheduler.ReduceLROnPlateau(
                    optimizer, mode="min", factor=factor,
                    patience=patience, min_lr=min_lr)
            elif stype == "cosine":
                return optim.lr_scheduler.CosineAnnealingLR(
                    optimizer, T_max=epochs, eta_min=min_lr)
            elif stype == "step":
                return optim.lr_scheduler.StepLR(
                    optimizer, step_size=patience, gamma=factor)
            elif stype == "exponential":
                return optim.lr_scheduler.ExponentialLR(
                    optimizer, gamma=factor)
            else:
                raise ValueError(f"Unknown scheduler_type: {scheduler_type}")

        def get_optimizer(model, cfg):
            lr = cfg["lr"]
            wd = cfg["weight_decay"]
            t  = cfg["type"].lower()
            if t == "adam":
                return optim.Adam(model.parameters(),    lr=lr, weight_decay=wd)
            elif t == "adamw":
                return optim.AdamW(model.parameters(),   lr=lr, weight_decay=wd)
            elif t == "sgd":
                return optim.SGD(model.parameters(),     lr=lr, weight_decay=wd, momentum=0.9, nesterov=True)
            elif t == "rmsprop":
                return optim.RMSprop(model.parameters(), lr=lr, weight_decay=wd, momentum=0.9)
            else:
                raise ValueError(f"Unknown optimizer type: {t}")

        # ============================================
        # Save best checkpoint helper
        # ============================================
        def save_checkpoint(path, model, optimizer, epoch, train_loss, val_loss,
                            train_recon, train_kl, val_recon, val_kl,
                            model_config, optimizer_config, best_val_loss, kl_weight_used):
            torch.save({
                "epoch":                epoch,
                "model_state_dict":     model.state_dict(),
                "optimizer_state_dict": optimizer.state_dict(),
                "train_loss":           train_loss,
                "val_loss":             val_loss,
                "train_recon_loss":     train_recon,
                "train_kl_loss":        train_kl,
                "val_recon_loss":       val_recon,
                "val_kl_loss":          val_kl,
                "best_val_loss":        best_val_loss,
                "kl_weight_used":       kl_weight_used,
                #  model_config always travels with every checkpoint
                "model_config":         model_config,
                "optimizer_config":     optimizer_config,
            }, path)

        # ============================================
        # Main
        # ============================================
        try:
            set_seed(args.seed)

            device = "cuda" if torch.cuda.is_available() else "cpu"
            if torch.cuda.is_available():
                logger.info(f"✓ GPU: {torch.cuda.get_device_name(0)}")
                logger.info(f"  Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
            else:
                logger.info("⚠ No GPU detected, using CPU")

            # ============================================
            # Load Init Checkpoint
            # ============================================
            logger.info("=" * 60)
            logger.info("Loading Init Checkpoint")
            logger.info("=" * 60)

            init_files = list(Path(args.model_input).glob("*.pt"))
            if not init_files:
                raise FileNotFoundError(f"No .pt file found in {args.model_input}")
            init_path  = init_files[0]
            logger.info(f"Init checkpoint: {init_path}")

            init_ckpt        = torch.load(init_path, map_location=device)
            model_config     = init_ckpt["model_config"]
            optimizer_config = init_ckpt["optimizer_config"]

            logger.info(f"  input_dim          : {model_config['input_dim']}")
            logger.info(f"  latent_dim         : {model_config['latent_dim']}")
            logger.info(f"  encoder_layers     : {model_config['encoder']['layer_sizes']}")
            logger.info(f"  decoder_layers     : {model_config['decoder']['layer_sizes']}")
            logger.info(f"  encoder_activation : {model_config['encoder']['activation']}")
            logger.info(f"  decoder_activation : {model_config['decoder']['activation']}")
            logger.info(f"  noise_type         : {model_config['noise']['noise_type']}")
            logger.info(f"  noise_factor       : {model_config['noise']['noise_factor']}")
            logger.info(f"  kl_weight          : {model_config['vae']['kl_weight']}")
            logger.info(f"  recon_loss_type    : {model_config['vae']['recon_loss_type']}")

            # ============================================
            # Reconstruct Model
            # ============================================
            logger.info("=" * 60)
            logger.info("Reconstructing NoisyVAE")
            logger.info("=" * 60)

            enc_cfg = model_config["encoder"]
            dec_cfg = model_config["decoder"]

            model = NoisyVAE(
                input_dim          = model_config["input_dim"],
                encoder_dims       = enc_cfg["layer_sizes"],
                decoder_dims       = dec_cfg["layer_sizes"],
                latent_dim         = model_config["latent_dim"],
                encoder_dropout    = enc_cfg["dropout"],
                decoder_dropout    = dec_cfg["dropout"],
                encoder_activation = enc_cfg["activation"],
                decoder_activation = dec_cfg["activation"],
                encoder_batch_norm = enc_cfg["batch_norm"],
                decoder_batch_norm = dec_cfg["batch_norm"],
            )
            model.load_state_dict(init_ckpt["model_state_dict"])
            model = model.to(device)
            logger.info(f"✓ Model on {device}")
            logger.info(f"  Total parameters : {sum(p.numel() for p in model.parameters()):,}")

            # ============================================
            # Optimizer & Scheduler
            # ============================================
            optimizer = get_optimizer(model, optimizer_config)
            optimizer.load_state_dict(init_ckpt["optimizer_state_dict"])
            scheduler = setup_scheduler(optimizer,
                                        args.scheduler_type,
                                        args.scheduler_factor,
                                        args.scheduler_patience,
                                        args.scheduler_min_lr,
                                        args.epochs)
            logger.info(f"✓ Optimizer : {optimizer_config['type']}  lr={optimizer_config['lr']}")
            logger.info(f"✓ Scheduler : {args.scheduler_type}")

            # ============================================
            # Loss function
            # ============================================
            recon_loss_fn   = get_recon_loss_fn(model_config["vae"]["recon_loss_type"])
            base_kl_weight  = model_config["vae"]["kl_weight"]
            noise_type      = model_config["noise"]["noise_type"]
            noise_factor    = model_config["noise"]["noise_factor"]

            logger.info(f"✓ Recon loss    : {model_config['vae']['recon_loss_type']}")
            logger.info(f"✓ KL weight     : {base_kl_weight}")
            logger.info(f"✓ KL annealing  : {args.kl_annealing_epochs} epochs")
            logger.info(f"✓ Noise type    : {noise_type}")
            logger.info(f"✓ Noise factor  : {noise_factor}")

            # ============================================
            # Load Datasets
            # ============================================
            logger.info("=" * 60)
            logger.info("Loading Datasets")
            logger.info("=" * 60)

            def load_dataset(path, split_name):
                pt_files = list(Path(path).glob("*.pt"))
                if not pt_files:
                    raise FileNotFoundError(f"No .pt file in {path}")
                data = torch.load(pt_files[0], map_location="cpu")
                if isinstance(data, dict):
                    tensor = data.get("X", data.get("data", list(data.values())[0]))
                else:
                    tensor = data
                if not isinstance(tensor, torch.Tensor):
                    tensor = torch.FloatTensor(np.array(tensor))
                tensor = tensor.float()
                logger.info(f"✓ {split_name}: {tensor.shape}")
                return TensorDataset(tensor)

            train_ds = load_dataset(args.train_dataset, "Train")
            val_ds   = load_dataset(args.val_dataset,   "Val")

            train_loader = DataLoader(train_ds, batch_size=args.batch_size, shuffle=True,
                                      num_workers=args.num_workers,
                                      pin_memory=(device == "cuda"))
            val_loader   = DataLoader(val_ds,   batch_size=args.batch_size, shuffle=False,
                                      num_workers=args.num_workers,
                                      pin_memory=(device == "cuda"))

            logger.info(f"  Train batches : {len(train_loader)}")
            logger.info(f"  Val batches   : {len(val_loader)}")

            # ============================================
            # Training Loop
            # ============================================
            logger.info("=" * 60)
            logger.info("Starting Training")
            logger.info("=" * 60)
            logger.info(f"  Epochs                  : {args.epochs}")
            logger.info(f"  Batch size              : {args.batch_size}")
            logger.info(f"  Gradient clip           : {args.gradient_clip_enabled}  max_norm={args.gradient_clip_max_norm}")
            logger.info(f"  Early stopping patience : {args.early_stopping_patience}")
            logger.info("=" * 60)

            os.makedirs(args.trained_model, exist_ok=True)
            best_model_path = os.path.join(args.trained_model, "best_trained_model.pth")

            history = {
                "train_loss": [], "val_loss": [],
                "train_recon_loss": [], "val_recon_loss": [],
                "train_kl_loss":    [], "val_kl_loss":    [],
                "kl_weights":       [],
                "learning_rates":   [],
                "epoch_times":      [],
            }

            best_val_loss    = float("inf")
            patience_counter = 0
            grad_clip        = str_to_bool(args.gradient_clip_enabled)

            for epoch in range(args.epochs):
                epoch_start = time.time()

                # ---- KL annealing: linear ramp 0 -> base_kl_weight ----
                if args.kl_annealing_epochs > 0:
                    kl_weight = min(base_kl_weight,
                                    base_kl_weight * (epoch + 1) / args.kl_annealing_epochs)
                else:
                    kl_weight = base_kl_weight

                # ---- Train ----
                model.train()
                t_losses, t_recons, t_kls = [], [], []

                for batch_idx, (x_clean,) in enumerate(train_loader):
                    x_clean = x_clean.to(device)

                    # Inject noise to input before encoding
                    x_noisy = inject_noise(x_clean, noise_type, noise_factor)

                    recon, mu, log_var, z = model(x_noisy)
                    loss, recon_l, kl_l   = vae_loss(recon, x_clean, mu, log_var,
                                                      recon_loss_fn, kl_weight)
                    optimizer.zero_grad()
                    loss.backward()

                    if grad_clip:
                        nn.utils.clip_grad_norm_(model.parameters(), args.gradient_clip_max_norm)

                    optimizer.step()

                    t_losses.append(loss.item())
                    t_recons.append(recon_l.item())
                    t_kls.append(kl_l.item())

                    if args.print_every_n_batches > 0 and (batch_idx + 1) % args.print_every_n_batches == 0:
                        logger.info(f"  Epoch {epoch+1} | Batch {batch_idx+1}/{len(train_loader)} | "
                                    f"loss={loss.item():.4f}  recon={recon_l.item():.4f}  kl={kl_l.item():.4f}")

                avg_train_loss  = float(np.mean(t_losses))
                avg_train_recon = float(np.mean(t_recons))
                avg_train_kl    = float(np.mean(t_kls))

                # ---- Validate ----
                model.eval()
                v_losses, v_recons, v_kls = [], [], []

                with torch.no_grad():
                    for (x_clean,) in val_loader:
                        x_clean = x_clean.to(device)
                        x_noisy = inject_noise(x_clean, noise_type, noise_factor)
                        recon, mu, log_var, z = model(x_noisy)
                        loss, recon_l, kl_l   = vae_loss(recon, x_clean, mu, log_var,
                                                          recon_loss_fn, kl_weight)
                        v_losses.append(loss.item())
                        v_recons.append(recon_l.item())
                        v_kls.append(kl_l.item())

                avg_val_loss  = float(np.mean(v_losses))
                avg_val_recon = float(np.mean(v_recons))
                avg_val_kl    = float(np.mean(v_kls))
                epoch_time    = time.time() - epoch_start
                current_lr    = optimizer.param_groups[0]["lr"]

                # Record history
                history["train_loss"].append(avg_train_loss)
                history["val_loss"].append(avg_val_loss)
                history["train_recon_loss"].append(avg_train_recon)
                history["val_recon_loss"].append(avg_val_recon)
                history["train_kl_loss"].append(avg_train_kl)
                history["val_kl_loss"].append(avg_val_kl)
                history["kl_weights"].append(kl_weight)
                history["learning_rates"].append(current_lr)
                history["epoch_times"].append(epoch_time)

                logger.info(f"Epoch [{epoch+1}/{args.epochs}] "
                            f"train={avg_train_loss:.6f} (recon={avg_train_recon:.4f} kl={avg_train_kl:.4f}) | "
                            f"val={avg_val_loss:.6f} (recon={avg_val_recon:.4f} kl={avg_val_kl:.4f}) | "
                            f"kl_w={kl_weight:.4f}  lr={current_lr:.6f}  t={epoch_time:.1f}s")

                # ---- Scheduler step ----
                if scheduler is not None:
                    if isinstance(scheduler, optim.lr_scheduler.ReduceLROnPlateau):
                        scheduler.step(avg_val_loss)
                    else:
                        scheduler.step()

                # ---- Best model check ----
                if avg_val_loss < best_val_loss - args.early_stopping_min_delta:
                    best_val_loss    = avg_val_loss
                    patience_counter = 0
                    save_checkpoint(best_model_path, model, optimizer,
                                    epoch + 1, avg_train_loss, avg_val_loss,
                                    avg_train_recon, avg_train_kl,
                                    avg_val_recon, avg_val_kl,
                                    model_config, optimizer_config,
                                    best_val_loss, kl_weight)
                    logger.info(f"✓ Best model saved  (val_loss={best_val_loss:.6f})")
                else:
                    patience_counter += 1
                    logger.info(f"  No improvement. Patience {patience_counter}/{args.early_stopping_patience}")

                # ---- Periodic checkpoint ----
                if args.save_every_n_epochs > 0 and (epoch + 1) % args.save_every_n_epochs == 0:
                    ckpt_dir  = os.path.join(args.trained_model, "checkpoints")
                    os.makedirs(ckpt_dir, exist_ok=True)
                    ckpt_path = os.path.join(ckpt_dir, f"checkpoint_epoch_{epoch+1}.pth")
                    save_checkpoint(ckpt_path, model, optimizer,
                                    epoch + 1, avg_train_loss, avg_val_loss,
                                    avg_train_recon, avg_train_kl,
                                    avg_val_recon, avg_val_kl,
                                    model_config, optimizer_config,
                                    best_val_loss, kl_weight)
                    logger.info(f"✓ Periodic checkpoint saved: {ckpt_path}")

                # ---- Early stopping ----
                if patience_counter >= args.early_stopping_patience:
                    logger.info(f"Early stopping triggered at epoch {epoch+1}")
                    break

            logger.info("=" * 60)
            logger.info("Training Complete!")
            logger.info("=" * 60)
            logger.info(f"  Best val loss    : {best_val_loss:.6f}")
            logger.info(f"  Epochs trained   : {len(history['train_loss'])}")
            logger.info(f"  Total time       : {sum(history['epoch_times']):.1f}s")
            logger.info(f"  Best model path  : {best_model_path}")

            # ============================================
            # Save Training History
            # ============================================
            os.makedirs(args.training_history, exist_ok=True)
            history_dict = {
                "train_loss":       [float(x) for x in history["train_loss"]],
                "val_loss":         [float(x) for x in history["val_loss"]],
                "train_recon_loss": [float(x) for x in history["train_recon_loss"]],
                "val_recon_loss":   [float(x) for x in history["val_recon_loss"]],
                "train_kl_loss":    [float(x) for x in history["train_kl_loss"]],
                "val_kl_loss":      [float(x) for x in history["val_kl_loss"]],
                "kl_weights":       [float(x) for x in history["kl_weights"]],
                "learning_rates":   [float(x) for x in history["learning_rates"]],
                "epoch_times":      [float(x) for x in history["epoch_times"]],
                "epochs_trained":   len(history["train_loss"]),
                "best_val_loss":    float(best_val_loss),
                "model_config": {
                    "input_dim":       model_config["input_dim"],
                    "latent_dim":      model_config["latent_dim"],
                    "encoder_layers":  model_config["encoder"]["layer_sizes"],
                    "decoder_layers":  model_config["decoder"]["layer_sizes"],
                    "encoder_act":     model_config["encoder"]["activation"],
                    "decoder_act":     model_config["decoder"]["activation"],
                    "noise_type":      model_config["noise"]["noise_type"],
                    "noise_factor":    model_config["noise"]["noise_factor"],
                    "kl_weight":       model_config["vae"]["kl_weight"],
                    "recon_loss_type": model_config["vae"]["recon_loss_type"],
                },
                "training_config": {
                    "epochs":                   args.epochs,
                    "batch_size":               args.batch_size,
                    "scheduler_type":           args.scheduler_type,
                    "kl_annealing_epochs":      args.kl_annealing_epochs,
                    "early_stopping_patience":  args.early_stopping_patience,
                    "gradient_clip_enabled":    args.gradient_clip_enabled,
                    "gradient_clip_max_norm":   args.gradient_clip_max_norm,
                }
            }

            history_json_path = os.path.join(args.training_history, "training_history.json")
            with open(history_json_path, "w") as f:
                json.dump(history_dict, f, indent=2)
            logger.info(f"✓ History JSON saved : {history_json_path}")

            history_npz_path = os.path.join(args.training_history, "training_history.npz")
            np.savez(history_npz_path,
                     train_loss       = np.array(history["train_loss"]),
                     val_loss         = np.array(history["val_loss"]),
                     train_recon_loss = np.array(history["train_recon_loss"]),
                     val_recon_loss   = np.array(history["val_recon_loss"]),
                     train_kl_loss    = np.array(history["train_kl_loss"]),
                     val_kl_loss      = np.array(history["val_kl_loss"]),
                     kl_weights       = np.array(history["kl_weights"]),
                     learning_rates   = np.array(history["learning_rates"]),
                     epoch_times      = np.array(history["epoch_times"]))
            logger.info(f"✓ History npz saved  : {history_npz_path}")

            with open(args.trained_model + ".meta.json", "w") as f:
                json.dump({"model_path":     best_model_path,
                           "best_val_loss":  float(best_val_loss),
                           "epochs_trained": len(history["train_loss"])}, f, indent=2)

        except Exception as e:
            logger.exception(f"Fatal error during training: {str(e)}")
            sys.exit(1)

    args:
      - --model_input
      - {inputPath: model_input}
      - --train_dataset
      - {inputPath: train_dataset}
      - --val_dataset
      - {inputPath: val_dataset}
      - --epochs
      - {inputValue: epochs}
      - --batch_size
      - {inputValue: batch_size}
      - --num_workers
      - {inputValue: num_workers}
      - --kl_annealing_epochs
      - {inputValue: kl_annealing_epochs}
      - --gradient_clip_enabled
      - {inputValue: gradient_clip_enabled}
      - --gradient_clip_max_norm
      - {inputValue: gradient_clip_max_norm}
      - --early_stopping_patience
      - {inputValue: early_stopping_patience}
      - --early_stopping_min_delta
      - {inputValue: early_stopping_min_delta}
      - --scheduler_type
      - {inputValue: scheduler_type}
      - --scheduler_factor
      - {inputValue: scheduler_factor}
      - --scheduler_patience
      - {inputValue: scheduler_patience}
      - --scheduler_min_lr
      - {inputValue: scheduler_min_lr}
      - --save_every_n_epochs
      - {inputValue: save_every_n_epochs}
      - --print_every_n_batches
      - {inputValue: print_every_n_batches}
      - --seed
      - {inputValue: seed}
      - --trained_model
      - {outputPath: trained_model}
      - --training_history
      - {outputPath: training_history}
