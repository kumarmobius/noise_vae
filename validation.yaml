name: Variational Autoencoder with Noise Injection Evaluator V1.1
description: Evaluate NoisyVAE on test set. Reports ELBO, reconstruction loss, KL divergence, R2, MAE, MSE, cosine similarity, SNR, and latent space statistics. Reads model_config directly from trained checkpoint.

inputs:
  - {name: trained_model, type: Model, description: "Trained VAE checkpoint from trainer (.pth)"}
  - {name: test_dataset,  type: Data,  description: "Test dataset (.pt tensor file)"}
  - {name: batch_size,    type: Integer, description: "Batch size for evaluation",   default: "32"}
  - {name: num_workers,   type: Integer, description: "DataLoader workers",           default: "0"}
  - {name: compute_per_sample_metrics, type: String,  description: "Compute per-sample statistics (true/false)", default: "true"}
  - {name: save_reconstructions,       type: String,  description: "Save reconstruction samples (true/false)",  default: "true"}
  - {name: num_reconstruction_samples, type: Integer, description: "Number of reconstruction samples to save",  default: "100"}
  - {name: seed, type: Integer, description: "Random seed", default: "42"}

outputs:
  - {name: evaluation_results,     type: Data,   description: "Full evaluation metrics JSON"}
  - {name: reconstruction_samples, type: Data,   description: "Saved input/noisy/recon/latent tensors"}
  - {name: schema_json,            type: String, description: "Schema JSON with key metrics"}

implementation:
  container:
    image: gurpreetgandhi/nesy-factory:v1
    command:
      - python3
      - -u
      - -c
      - |
        import argparse
        import os
        import sys
        import json
        import logging
        import random
        import time
        import numpy as np
        import torch
        import torch.nn as nn
        from torch.utils.data import DataLoader, TensorDataset
        from pathlib import Path
        from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

        logging.basicConfig(stream=sys.stdout, level=logging.INFO)
        logger = logging.getLogger("vae_evaluator")

        parser = argparse.ArgumentParser()
        parser.add_argument("--trained_model",              type=str,   required=True)
        parser.add_argument("--test_dataset",               type=str,   required=True)
        parser.add_argument("--batch_size",                 type=int,   required=True)
        parser.add_argument("--num_workers",                type=int,   required=True)
        parser.add_argument("--compute_per_sample_metrics", type=str,   required=True)
        parser.add_argument("--save_reconstructions",       type=str,   required=True)
        parser.add_argument("--num_reconstruction_samples", type=int,   required=True)
        parser.add_argument("--seed",                       type=int,   required=True)
        parser.add_argument("--evaluation_results",         type=str,   required=True)
        parser.add_argument("--reconstruction_samples",     type=str,   required=True)
        parser.add_argument("--schema_json",                type=str,   required=True)
        args = parser.parse_args()

        # ============================================
        # Helpers
        # ============================================
        def str_to_bool(s):
            return str(s).lower() in ["true", "1", "yes"]

        def ensure_dir_for(p):
            d = os.path.dirname(p)
            if d and not os.path.exists(d):
                os.makedirs(d, exist_ok=True)

        def set_seed(seed):
            random.seed(seed)
            np.random.seed(seed)
            torch.manual_seed(seed)
            torch.cuda.manual_seed(seed)
            torch.cuda.manual_seed_all(seed)
            torch.backends.cudnn.deterministic = True
            torch.backends.cudnn.benchmark = False
            logger.info(f"✓ Seed set: {seed}")

        def get_activation(name):
            activations = {
                "relu":       nn.ReLU(),
                "leaky_relu": nn.LeakyReLU(0.2),
                "elu":        nn.ELU(),
                "selu":       nn.SELU(),
                "gelu":       nn.GELU(),
                "tanh":       nn.Tanh(),
                "sigmoid":    nn.Sigmoid(),
                "swish":      nn.SiLU(),
                "mish":       nn.Mish(),
                "prelu":      nn.PReLU(),
            }
            act = activations.get(name.lower())
            if act is None:
                raise ValueError(f"Unknown activation '{name}'")
            return act

        # ============================================
        # Model Classes — must match builder exactly
        # ============================================
        class VAEEncoder(nn.Module):
            def __init__(self, input_dim, encoder_dims, latent_dim,
                         dropout, activation, use_batch_norm):
                super(VAEEncoder, self).__init__()
                layers   = []
                prev_dim = input_dim
                for idx, units in enumerate(encoder_dims):
                    layers.append(nn.Linear(prev_dim, units))
                    if use_batch_norm:
                        layers.append(nn.BatchNorm1d(units))
                    layers.append(get_activation(activation))
                    if dropout > 0:
                        layers.append(nn.Dropout(dropout))
                    logger.info(f"    Encoder Layer {idx + 1}: {prev_dim} -> {units}")
                    prev_dim = units
                self.shared     = nn.Sequential(*layers)
                self.fc_mu      = nn.Linear(prev_dim, latent_dim)
                self.fc_log_var = nn.Linear(prev_dim, latent_dim)
                logger.info(f"    Encoder mu/log_var : {prev_dim} -> {latent_dim}")

            def forward(self, x):
                h = self.shared(x)
                return self.fc_mu(h), self.fc_log_var(h)

        class VAEDecoder(nn.Module):
            def __init__(self, latent_dim, decoder_dims, input_dim,
                         dropout, activation, use_batch_norm):
                super(VAEDecoder, self).__init__()
                layers   = []
                prev_dim = latent_dim
                for idx, units in enumerate(decoder_dims):
                    layers.append(nn.Linear(prev_dim, units))
                    if use_batch_norm:
                        layers.append(nn.BatchNorm1d(units))
                    layers.append(get_activation(activation))
                    if dropout > 0:
                        layers.append(nn.Dropout(dropout))
                    logger.info(f"    Decoder Layer {idx + 1}: {prev_dim} -> {units}")
                    prev_dim = units
                layers.append(nn.Linear(prev_dim, input_dim))
                logger.info(f"    Decoder Output : {prev_dim} -> {input_dim}")
                self.decoder = nn.Sequential(*layers)

            def forward(self, z):
                return self.decoder(z)

        class NoisyVAE(nn.Module):
            def __init__(self, input_dim, encoder_dims, decoder_dims, latent_dim,
                         encoder_dropout, decoder_dropout,
                         encoder_activation, decoder_activation,
                         encoder_batch_norm, decoder_batch_norm):
                super(NoisyVAE, self).__init__()
                self.input_dim  = input_dim
                self.latent_dim = latent_dim
                self.encoder = VAEEncoder(input_dim, encoder_dims, latent_dim,
                                          encoder_dropout, encoder_activation, encoder_batch_norm)
                self.decoder = VAEDecoder(latent_dim, decoder_dims, input_dim,
                                          decoder_dropout, decoder_activation, decoder_batch_norm)

            def reparameterize(self, mu, log_var):
                if self.training:
                    std = torch.exp(0.5 * log_var)
                    return mu + torch.randn_like(std) * std
                return mu  # deterministic at eval

            def forward(self, x):
                mu, log_var = self.encoder(x)
                z           = self.reparameterize(mu, log_var)
                recon       = self.decoder(z)
                return recon, mu, log_var, z

            def encode(self, x):
                mu, log_var = self.encoder(x)
                return self.reparameterize(mu, log_var), mu, log_var

            def decode(self, z):
                return self.decoder(z)

            def sample(self, n, device):
                return self.decoder(torch.randn(n, self.latent_dim, device=device))

        # ============================================
        # Noise Injection (same as trainer)
        # ============================================
        def inject_noise(x, noise_type, noise_factor):
            if noise_factor == 0.0:
                return x
            if noise_type == "gaussian":
                return x + torch.randn_like(x) * noise_factor
            elif noise_type == "dropout":
                return x * (torch.rand_like(x) > noise_factor)
            elif noise_type == "uniform":
                return x + (torch.rand_like(x) - 0.5) * 2 * noise_factor
            elif noise_type == "salt_pepper":
                noisy = x.clone()
                mask  = torch.rand_like(x) < noise_factor
                vals  = torch.rand(mask.sum(), device=x.device)
                vals  = vals * (x.max() - x.min()) + x.min()
                noisy[mask] = vals
                return noisy
            else:
                raise ValueError(f"Unknown noise_type: {noise_type}")

        # ============================================
        # VAE Loss (same as trainer)
        # ============================================
        def get_recon_loss_fn(loss_type):
            fns = {
                "mse":       nn.MSELoss(reduction="sum"),
                "l1":        nn.L1Loss(reduction="sum"),
                "smooth_l1": nn.SmoothL1Loss(reduction="sum"),
                "huber":     nn.HuberLoss(reduction="sum", delta=1.0),
            }
            fn = fns.get(loss_type.lower())
            if fn is None:
                raise ValueError(f"Unknown recon_loss_type: {loss_type}")
            return fn

        def vae_loss(recon, x, mu, log_var, recon_loss_fn, kl_weight):
            batch_size = x.size(0)
            recon_loss = recon_loss_fn(recon, x) / batch_size
            kl_loss    = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp()) / batch_size
            total      = recon_loss + kl_weight * kl_loss
            return total, recon_loss, kl_loss

        # ============================================
        # Reconstruction Metrics
        # ============================================
        def compute_reconstruction_metrics(original_np, reconstructed_np):
            o = original_np.flatten()
            r = reconstructed_np.flatten()
            metrics = {}
            metrics["mse"]      = float(mean_squared_error(o, r))
            metrics["rmse"]     = float(np.sqrt(metrics["mse"]))
            metrics["mae"]      = float(mean_absolute_error(o, r))
            metrics["r2_score"] = float(r2_score(o, r))

            per_sample_mse = np.mean((original_np - reconstructed_np) ** 2, axis=1)
            metrics["per_sample_mse_mean"]   = float(np.mean(per_sample_mse))
            metrics["per_sample_mse_std"]    = float(np.std(per_sample_mse))
            metrics["per_sample_mse_min"]    = float(np.min(per_sample_mse))
            metrics["per_sample_mse_max"]    = float(np.max(per_sample_mse))
            metrics["per_sample_mse_median"] = float(np.median(per_sample_mse))

            # Cosine similarity
            from sklearn.metrics.pairwise import cosine_similarity
            cos_sims = [cosine_similarity(original_np[i:i+1], reconstructed_np[i:i+1])[0, 0]
                        for i in range(len(original_np))]
            metrics["cosine_similarity_mean"] = float(np.mean(cos_sims))
            metrics["cosine_similarity_std"]  = float(np.std(cos_sims))

            # Signal-to-noise ratio
            signal_power = np.mean(original_np ** 2)
            noise_power  = np.mean((original_np - reconstructed_np) ** 2)
            metrics["snr_db"] = float(10 * np.log10(signal_power / noise_power)) if noise_power > 0 else float("inf")

            return metrics

        # ============================================
        # Latent Space Statistics
        # ============================================
        def compute_latent_stats(mu_np, log_var_np, z_np):
            var_np = np.exp(log_var_np)
            return {
                # mu stats
                "mu_mean":           float(np.mean(mu_np)),
                "mu_std":            float(np.std(mu_np)),
                "mu_abs_mean":       float(np.mean(np.abs(mu_np))),
                # variance stats
                "var_mean":          float(np.mean(var_np)),
                "var_std":           float(np.std(var_np)),
                # KL per dimension (analytical)
                "kl_per_dim_mean":   float(np.mean(0.5 * (mu_np**2 + var_np - np.log(var_np) - 1))),
                # z stats
                "z_mean":            float(np.mean(z_np)),
                "z_std":             float(np.std(z_np)),
                # Active dimensions: latent dims NOT collapsed to prior
                "active_dims":       int(np.sum(np.var(mu_np, axis=0) > 0.01)),
                "total_dims":        int(mu_np.shape[1]),
                "active_dims_ratio": float(np.sum(np.var(mu_np, axis=0) > 0.01) / mu_np.shape[1]),
            }

        # ============================================
        # Main
        # ============================================
        try:
            set_seed(args.seed)

            device = "cuda" if torch.cuda.is_available() else "cpu"
            if torch.cuda.is_available():
                logger.info(f"✓ GPU: {torch.cuda.get_device_name(0)}")
            else:
                logger.info("⚠ No GPU, using CPU")

            # ============================================
            # Load Checkpoint
            # ============================================
            logger.info("=" * 60)
            logger.info("Loading Trained Model")
            logger.info("=" * 60)

            model_files = list(Path(args.trained_model).glob("*.pth"))
            if not model_files:
                raise FileNotFoundError(f"No .pth file found in {args.trained_model}")

            checkpoint = torch.load(model_files[0], map_location=device)
            logger.info(f"✓ Loaded: {model_files[0]}")
            logger.info(f"  Epoch          : {checkpoint['epoch']}")
            logger.info(f"  Train loss     : {checkpoint['train_loss']:.6f}")
            logger.info(f"  Val loss       : {checkpoint['val_loss']:.6f}")
            logger.info(f"  Train recon    : {checkpoint['train_recon_loss']:.6f}")
            logger.info(f"  Train KL       : {checkpoint['train_kl_loss']:.6f}")
            logger.info(f"  Val recon      : {checkpoint['val_recon_loss']:.6f}")
            logger.info(f"  Val KL         : {checkpoint['val_kl_loss']:.6f}")
            logger.info(f"  KL weight used : {checkpoint['kl_weight_used']:.4f}")

            # ✅ Read model_config directly from checkpoint — no builder file needed
            if "model_config" not in checkpoint:
                raise KeyError(
                    "model_config not found in checkpoint. "
                    "Retrain using the updated trainer YAML which saves model_config inside the checkpoint."
                )

            model_config     = checkpoint["model_config"]
            optimizer_config = checkpoint.get("optimizer_config", {})

            enc_cfg = model_config["encoder"]
            dec_cfg = model_config["decoder"]

            logger.info(f"  input_dim          : {model_config['input_dim']}")
            logger.info(f"  latent_dim         : {model_config['latent_dim']}")
            logger.info(f"  encoder_layers     : {enc_cfg['layer_sizes']}")
            logger.info(f"  decoder_layers     : {dec_cfg['layer_sizes']}")
            logger.info(f"  encoder_activation : {enc_cfg['activation']}")
            logger.info(f"  decoder_activation : {dec_cfg['activation']}")
            logger.info(f"  noise_type         : {model_config['noise']['noise_type']}")
            logger.info(f"  noise_factor       : {model_config['noise']['noise_factor']}")
            logger.info(f"  kl_weight          : {model_config['vae']['kl_weight']}")
            logger.info(f"  recon_loss_type    : {model_config['vae']['recon_loss_type']}")

            # ============================================
            # Reconstruct Model
            # ============================================
            logger.info("=" * 60)
            logger.info("Reconstructing NoisyVAE")
            logger.info("=" * 60)

            model = NoisyVAE(
                input_dim          = model_config["input_dim"],
                encoder_dims       = enc_cfg["layer_sizes"],
                decoder_dims       = dec_cfg["layer_sizes"],
                latent_dim         = model_config["latent_dim"],
                encoder_dropout    = enc_cfg["dropout"],
                decoder_dropout    = dec_cfg["dropout"],
                encoder_activation = enc_cfg["activation"],
                decoder_activation = dec_cfg["activation"],
                encoder_batch_norm = enc_cfg["batch_norm"],
                decoder_batch_norm = dec_cfg["batch_norm"],
            )
            model.load_state_dict(checkpoint["model_state_dict"])
            model = model.to(device)
            model.eval()  # deterministic: reparameterize returns mu directly

            logger.info(f"✓ Model on {device}")
            logger.info(f"  Total parameters : {sum(p.numel() for p in model.parameters()):,}")

            # ============================================
            # Load Test Dataset
            # ============================================
            logger.info("=" * 60)
            logger.info("Loading Test Dataset")
            logger.info("=" * 60)

            pt_files = list(Path(args.test_dataset).glob("*.pt"))
            if not pt_files:
                raise FileNotFoundError(f"No .pt file in {args.test_dataset}")

            raw = torch.load(pt_files[0], map_location="cpu")
            if isinstance(raw, dict):
                test_tensor = raw.get("X", raw.get("data", list(raw.values())[0]))
            else:
                test_tensor = raw
            if not isinstance(test_tensor, torch.Tensor):
                test_tensor = torch.FloatTensor(np.array(test_tensor))
            test_tensor = test_tensor.float()

            logger.info(f"✓ Test dataset shape : {test_tensor.shape}")

            test_loader = DataLoader(
                TensorDataset(test_tensor),
                batch_size  = args.batch_size,
                shuffle     = False,
                num_workers = args.num_workers,
                pin_memory  = (device == "cuda")
            )
            logger.info(f"  Test batches : {len(test_loader)}")

            # ============================================
            # Evaluation Loop
            # ============================================
            logger.info("=" * 60)
            logger.info("Running Evaluation")
            logger.info("=" * 60)

            noise_type   = model_config["noise"]["noise_type"]
            noise_factor = model_config["noise"]["noise_factor"]
            kl_weight    = model_config["vae"]["kl_weight"]
            recon_loss_fn = get_recon_loss_fn(model_config["vae"]["recon_loss_type"])

            all_originals, all_noisy = [], []
            all_recons, all_mus, all_log_vars, all_zs = [], [], [], []
            batch_totals, batch_recons, batch_kls = [], [], []

            with torch.no_grad():
                for batch_idx, (x_clean,) in enumerate(test_loader):
                    x_clean = x_clean.to(device)
                    x_noisy = inject_noise(x_clean, noise_type, noise_factor)

                    recon, mu, log_var, z = model(x_noisy)  # reparameterize returns mu at eval
                    total, recon_l, kl_l  = vae_loss(recon, x_clean, mu, log_var,
                                                      recon_loss_fn, kl_weight)

                    all_originals.append(x_clean.cpu())
                    all_noisy.append(x_noisy.cpu())
                    all_recons.append(recon.cpu())
                    all_mus.append(mu.cpu())
                    all_log_vars.append(log_var.cpu())
                    all_zs.append(z.cpu())
                    batch_totals.append(total.item())
                    batch_recons.append(recon_l.item())
                    batch_kls.append(kl_l.item())

                    if (batch_idx + 1) % 10 == 0:
                        logger.info(f"  Processed {batch_idx + 1}/{len(test_loader)} batches")

            all_originals = torch.cat(all_originals, dim=0)
            all_noisy     = torch.cat(all_noisy,     dim=0)
            all_recons    = torch.cat(all_recons,    dim=0)
            all_mus       = torch.cat(all_mus,       dim=0)
            all_log_vars  = torch.cat(all_log_vars,  dim=0)
            all_zs        = torch.cat(all_zs,        dim=0)

            logger.info(f"✓ Evaluation complete — {len(all_originals)} samples")

            # ============================================
            # Compute Metrics
            # ============================================
            logger.info("=" * 60)
            logger.info("Computing Metrics")
            logger.info("=" * 60)

            # --- VAE losses ---
            avg_total_loss = float(np.mean(batch_totals))
            avg_recon_loss = float(np.mean(batch_recons))
            avg_kl_loss    = float(np.mean(batch_kls))
            elbo           = -(avg_recon_loss + avg_kl_loss)  # ELBO = -(recon + KL)

            logger.info(f"  ELBO            : {elbo:.6f}")
            logger.info(f"  Total VAE loss  : {avg_total_loss:.6f}")
            logger.info(f"  Recon loss      : {avg_recon_loss:.6f}")
            logger.info(f"  KL divergence   : {avg_kl_loss:.6f}")

            # --- Reconstruction quality ---
            orig_np  = all_originals.numpy()
            recon_np = all_recons.numpy()
            recon_metrics = compute_reconstruction_metrics(orig_np, recon_np)

            logger.info(f"  MSE             : {recon_metrics['mse']:.6f}")
            logger.info(f"  RMSE            : {recon_metrics['rmse']:.6f}")
            logger.info(f"  MAE             : {recon_metrics['mae']:.6f}")
            logger.info(f"  R² Score        : {recon_metrics['r2_score']:.6f}")
            logger.info(f"  Cosine Sim      : {recon_metrics['cosine_similarity_mean']:.6f}")
            logger.info(f"  SNR (dB)        : {recon_metrics['snr_db']:.2f}")

            # --- Latent space quality ---
            mu_np      = all_mus.numpy()
            log_var_np = all_log_vars.numpy()
            z_np       = all_zs.numpy()
            latent_stats = compute_latent_stats(mu_np, log_var_np, z_np)

            logger.info(f"  Active dims     : {latent_stats['active_dims']}/{latent_stats['total_dims']}  "
                        f"({latent_stats['active_dims_ratio']*100:.1f}%)")
            logger.info(f"  Mean mu         : {latent_stats['mu_mean']:.4f}  (want ~0)")
            logger.info(f"  Mean var        : {latent_stats['var_mean']:.4f}  (want ~1)")
            logger.info(f"  KL per dim      : {latent_stats['kl_per_dim_mean']:.4f}")

            # ---- Optional per-sample metrics ----
            per_sample_stats = None
            if str_to_bool(args.compute_per_sample_metrics):
                logger.info("  Computing per-sample statistics...")
                per_sample_mse  = np.mean((orig_np - recon_np) ** 2, axis=1)
                per_sample_mae  = np.mean(np.abs(orig_np - recon_np), axis=1)
                per_sample_rmse = np.sqrt(per_sample_mse)
                kl_per_sample   = -0.5 * np.sum(1 + log_var_np - mu_np**2 - np.exp(log_var_np), axis=1)

                def arr_stats(arr):
                    return {"mean": float(np.mean(arr)), "std": float(np.std(arr)),
                            "min":  float(np.min(arr)),  "max": float(np.max(arr)),
                            "median": float(np.median(arr))}

                per_sample_stats = {
                    "mse":        arr_stats(per_sample_mse),
                    "mae":        arr_stats(per_sample_mae),
                    "rmse":       arr_stats(per_sample_rmse),
                    "kl_per_sample": arr_stats(kl_per_sample),
                }
                logger.info(f"  Per-sample MSE  mean={per_sample_stats['mse']['mean']:.6f}  "
                            f"std={per_sample_stats['mse']['std']:.6f}")
                logger.info(f"  Per-sample KL   mean={per_sample_stats['kl_per_sample']['mean']:.6f}  "
                            f"std={per_sample_stats['kl_per_sample']['std']:.6f}")

            # ============================================
            # Save Evaluation Results JSON
            # ============================================
            logger.info("=" * 60)
            logger.info("Saving Results")
            logger.info("=" * 60)

            os.makedirs(args.evaluation_results, exist_ok=True)
            results = {
                "vae_losses": {
                    "elbo":             elbo,
                    "total_loss":       avg_total_loss,
                    "recon_loss":       avg_recon_loss,
                    "kl_loss":          avg_kl_loss,
                    "kl_weight_used":   float(checkpoint["kl_weight_used"]),
                },
                "reconstruction_metrics": recon_metrics,
                "latent_space_stats":     latent_stats,
                "per_sample_stats":       per_sample_stats,
                "test_samples":           int(len(all_originals)),
                "model_info": {
                    "input_dim":          model_config["input_dim"],
                    "latent_dim":         model_config["latent_dim"],
                    "encoder_layers":     enc_cfg["layer_sizes"],
                    "decoder_layers":     dec_cfg["layer_sizes"],
                    "encoder_activation": enc_cfg["activation"],
                    "decoder_activation": dec_cfg["activation"],
                    "noise_type":         model_config["noise"]["noise_type"],
                    "noise_factor":       model_config["noise"]["noise_factor"],
                    "kl_weight":          model_config["vae"]["kl_weight"],
                    "recon_loss_type":    model_config["vae"]["recon_loss_type"],
                    "trained_epoch":      int(checkpoint["epoch"]),
                    "best_val_loss":      float(checkpoint.get("best_val_loss", 0.0)),
                },
                "evaluation_config": {
                    "batch_size":  args.batch_size,
                    "device":      str(device),
                }
            }

            results_path = os.path.join(args.evaluation_results, "evaluation_results.json")
            with open(results_path, "w") as f:
                json.dump(results, f, indent=2)
            logger.info(f"✓ Evaluation results saved: {results_path}")

            with open(args.evaluation_results + ".meta.json", "w") as f:
                json.dump({"results_path": results_path,
                           "test_samples": len(all_originals),
                           "elbo":         elbo,
                           "mse":          recon_metrics["mse"]}, f, indent=2)

            # ============================================
            # Save Reconstruction Samples
            # ============================================
            if str_to_bool(args.save_reconstructions):
                os.makedirs(args.reconstruction_samples, exist_ok=True)
                n = min(args.num_reconstruction_samples, len(all_originals))
                samples_path = os.path.join(args.reconstruction_samples, "reconstruction_samples.pt")
                torch.save({
                    "original":     all_originals[:n],
                    "noisy":        all_noisy[:n],
                    "reconstructed":all_recons[:n],
                    "mu":           all_mus[:n],
                    "log_var":      all_log_vars[:n],
                    "z":            all_zs[:n],
                }, samples_path)
                logger.info(f"✓ Saved {n} reconstruction samples: {samples_path}")

                with open(args.reconstruction_samples + ".meta.json", "w") as f:
                    json.dump({"samples_path": samples_path, "num_samples": n}, f, indent=2)

            # ============================================
            # Save Schema JSON
            # ============================================
            ensure_dir_for(args.schema_json)
            schema_data = {
                "timestamp":       int(time.time()),
                "elbo":            float(elbo),
                "recon_loss":      float(avg_recon_loss),
                "kl_loss":         float(avg_kl_loss),
                "mse":             float(recon_metrics["mse"]),
                "active_dims":     int(latent_stats["active_dims"]),
            }
            with open(args.schema_json, "w") as f:
                json.dump(schema_data, f, indent=2)
            logger.info(f"✓ Schema JSON saved: {args.schema_json}")

            logger.info("=" * 60)
            logger.info("✓ Evaluation Complete!")
            logger.info("=" * 60)
            logger.info(f"  ELBO             : {elbo:.6f}")
            logger.info(f"  Recon loss       : {avg_recon_loss:.6f}")
            logger.info(f"  KL divergence    : {avg_kl_loss:.6f}")
            logger.info(f"  MSE              : {recon_metrics['mse']:.6f}")
            logger.info(f"  R² Score         : {recon_metrics['r2_score']:.6f}")
            logger.info(f"  Active dims      : {latent_stats['active_dims']}/{latent_stats['total_dims']}")

        except Exception as e:
            logger.exception(f"Fatal error during evaluation: {str(e)}")
            sys.exit(1)

    args:
      - --trained_model
      - {inputPath: trained_model}
      - --test_dataset
      - {inputPath: test_dataset}
      - --batch_size
      - {inputValue: batch_size}
      - --num_workers
      - {inputValue: num_workers}
      - --compute_per_sample_metrics
      - {inputValue: compute_per_sample_metrics}
      - --save_reconstructions
      - {inputValue: save_reconstructions}
      - --num_reconstruction_samples
      - {inputValue: num_reconstruction_samples}
      - --seed
      - {inputValue: seed}
      - --evaluation_results
      - {outputPath: evaluation_results}
      - --reconstruction_samples
      - {outputPath: reconstruction_samples}
      - --schema_json
      - {outputPath: schema_json}
